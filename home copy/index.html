<!doctype html>
<html data-n-head-ssr lang="en" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>Hao Yin - Personal Website</title><meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width,initial-scale=1"><meta data-n-head="ssr" name="keywords" content="Hao Yin, Yin Hao, 银皓, Northwestern University, Research"><meta data-n-head="ssr" name="description" content="Hao Yin (银皓)- personal research website. Hao Yin is a Ph.D. candidate of Civil Engineering working in the Multiscale Mechanics of Infrastructure Materials (M2IM) Lab at Northwestern University. He is currently completing his Ph.D. thesis, under the supervision of Dr. Gianluca Cusatis, focusing on the development of a novel computational framework for thermo-hygro-mechanical analyses of complex lattice systems (e.g., wood microstructure, bio-inspired composites). His research interests include lattice/discrete models, bio-inspired materials, fracture mechanics, multiphysics, and generative geometry.."><meta data-n-head="ssr" name="format-detection" content="telephone=no"><meta data-n-head="ssr" data-hid="charset" charset="utf-8"><meta data-n-head="ssr" data-hid="mobile-web-app-capable" name="mobile-web-app-capable" content="yes"><meta data-n-head="ssr" data-hid="apple-mobile-web-app-title" name="apple-mobile-web-app-title" content="yhwebsite"><meta data-n-head="ssr" data-hid="og:type" name="og:type" property="og:type" content="website"><meta data-n-head="ssr" data-hid="og:title" name="og:title" property="og:title" content="yhwebsite"><meta data-n-head="ssr" data-hid="og:site_name" name="og:site_name" property="og:site_name" content="yhwebsite"><meta data-n-head="ssr" data-hid="og:description" name="og:description" property="og:description" content="## Build Setup"><link data-n-head="ssr" rel="icon" type="image/x-icon" href="/favicon.ico"><link data-n-head="ssr" rel="stylesheet" href="/main.css"><link data-n-head="ssr" rel="stylesheet" href="/font.css"><link data-n-head="ssr" data-hid="shortcut-icon" rel="shortcut icon" href="/_nuxt/icons/icon_64x64.e3e9fb.png"><link data-n-head="ssr" data-hid="apple-touch-icon" rel="apple-touch-icon" href="/_nuxt/icons/icon_512x512.e3e9fb.png" sizes="512x512"><link data-n-head="ssr" rel="manifest" href="/_nuxt/manifest.153f5cc4.json" data-hid="manifest"><link rel="preload" href="/_nuxt/cd4766f.js" as="script"><link rel="preload" href="/_nuxt/1765d2c.js" as="script"><link rel="preload" href="/_nuxt/0dc6b56.js" as="script"><link rel="preload" href="/_nuxt/e86a509.js" as="script"><link rel="preload" href="/_nuxt/3c593d9.js" as="script"><style data-vue-ssr-id="2469b305:0 517a8dd7:0 1a73ea6b:0">.md-body{margin:0 auto;font-family:"Microsoft YaHei",arial,sans-serif;color:#444;line-height:1;padding:50px 30px 30px}.md-body h1,.md-body h2,.md-body h3,.md-body h4{color:#111;font-weight:400;margin-top:1em}.md-body h1,.md-body h2,.md-body h3,.md-body h4,.md-body h5{font-family:Georgia,Palatino,serif}.md-body dl,.md-body h1,.md-body h2,.md-body h3,.md-body h4,.md-body h5,.md-body p{margin-bottom:16px;padding:0}.md-body h1{font-size:48px;line-height:54px}.md-body h2{font-size:36px;line-height:42px}.md-body h1,.md-body h2{padding-bottom:10px}.md-body h3{font-size:24px;line-height:30px}.md-body h4{font-size:21px;line-height:26px}.md-body h5{font-size:18px;list-style:23px}.md-body a{color:#09f;margin:0;padding:0;vertical-align:baseline}.md-body a:hover{text-decoration:none;color:#f60}.md-body ol,.md-body ul{padding:0 0 0 24px;margin:0}.md-body li{line-height:24px}.md-body ol,.md-body p,.md-body ul{font-size:16px;line-height:24px}.md-body ol ol,.md-body ul ol{list-style-type:lower-roman}.md-body code,.md-body pre{border-radius:3px;background-color:#f7f7f7;color:inherit}.md-body code{font-family:Consolas,Monaco,Andale Mono,monospace;margin:0 2px}.md-body pre{line-height:1.7em;overflow:auto;padding:6px 10px;border-left:5px solid #6ce26c}.md-body pre>code{border:0;display:inline;max-width:none;padding:0;margin:0;overflow:visible;overflow:initial;line-height:inherit;font-size:.85em;white-space:pre;background:0 0}.md-body code{color:#666555}.md-body aside{display:block;float:right;width:390px}.md-body blockquote{border-left:.5em solid #eee;padding:0 0 0 2em;margin-left:0}.md-body blockquote cite{font-size:14px;line-height:20px;color:#bfbfbf}.md-body blockquote cite:before{content:"\2014 \00A0"}.md-body blockquote p{color:#666}.md-body hr{text-align:left;color:#999;height:2px;padding:0;margin:16px 0;background-color:#e7e7e7;border:0}.md-body dl{padding:0}.md-body dl dt{padding:10px 0;margin-top:16px;font-size:1em;font-style:italic;font-weight:700}.md-body dl dd{padding:0 16px;margin-bottom:16px}.md-body dd{margin-left:0}.md-body button,.md-body input,.md-body select,.md-body textarea{font-size:100%;margin:0;vertical-align:baseline}.md-body button,.md-body input{line-height:normal}.md-body button::-moz-focus-inner,.md-body input::-moz-focus-inner{border:0;padding:0}.md-body button,.md-body input[type=button],.md-body input[type=reset],.md-body input[type=submit]{cursor:pointer;-webkit-appearance:button}.md-body input:not([type=image]),.md-body textarea{box-sizing:content-box}.md-body input[type=search]{-webkit-appearance:textfield;box-sizing:content-box}.md-body input[type=search]::-webkit-search-decoration{-webkit-appearance:none}.md-body input,.md-body label,.md-body select,.md-body textarea{font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;font-size:13px;font-weight:400;line-height:normal;margin-bottom:18px}.md-body input[type=checkbox],.md-body input[type=radio]{cursor:pointer;margin-bottom:0}.md-body input[type=password],.md-body input[type=text],.md-body select,.md-body textarea{display:inline-block;width:210px;padding:4px;font-size:13px;font-weight:400;line-height:18px;height:18px;color:grey;border:1px solid #ccc;border-radius:3px}.md-body input[type=file],.md-body select{height:27px;line-height:27px}.md-body textarea{height:auto}.md-body :-moz-placeholder{color:#bfbfbf}.md-body ::-webkit-input-placeholder{color:#bfbfbf}.md-body input[type=password],.md-body input[type=text],.md-body select,.md-body textarea{transition:border .2s linear,box-shadow .2s linear;box-shadow:inset 0 1px 3px rgba(0,0,0,.1)}.md-body input[type=password]:focus,.md-body input[type=text]:focus,.md-body textarea:focus{outline:0;border-color:rgba(82,168,236,.8);box-shadow:inset 0 1px 3px rgba(0,0,0,.1),0 0 8px rgba(82,168,236,.6)}.md-body button{display:inline-block;padding:4px 14px;font-family:"Helvetica Neue",Helvetica,Arial,sans-serif;font-size:13px;line-height:18px;border-radius:4px;box-shadow:inset 0 1px 0 hsla(0,0%,100%,.2),0 1px 2px rgba(0,0,0,.05);background-color:#0064cd;background-repeat:repeat-x;background-image:-khtml-gradient(linear,left top,left bottom,from(#049cdb),to(#0064cd));background-image:linear-gradient(180deg,#049cdb,#0064cd);color:#fff;text-shadow:0 -1px 0 rgba(0,0,0,.25);transition:all .1s linear;border:1px solid rgba(0,0,0,.1);border-bottom-color:rgba(0,0,0,.25)}.md-body button:hover{color:#fff;background-position:0 -15px;text-decoration:none}.md-body button:active{box-shadow:inset 0 3px 7px rgba(0,0,0,.15),0 1px 2px rgba(0,0,0,.05)}.md-body button::-moz-focus-inner{padding:0;border:0}.md-body table{border-spacing:0;width:100%;border:1px solid #ccc;border-radius:6px}.md-body table tr:hover{background:#fbf8e9;transition:all .1s ease-in-out}.md-body .table th,.md-body table td{border-left:1px solid #ccc;border-top:1px solid #ccc;padding:10px;text-align:left}.md-body table th{background-color:#dce9f9;background-image:linear-gradient(180deg,#ebf3fc,#dce9f9);border-top:none;text-shadow:0 1px 0 hsla(0,0%,100%,.5);padding:5px}.md-body table td:first-child,.md-body table th:first-child{border-left:none}.md-body table th:first-child{border-radius:6px 0 0 0}.md-body table th:last-child{border-radius:0 6px 0 0}.md-body table th:only-child{border-radius:6px 6px 0 0}.md-body table tr:last-child td:first-child{border-radius:0 0 0 6px}.md-body table tr:last-child td:last-child{border-radius:0 0 6px 0}code[class*=language-],pre[class*=language-]{color:#000;background:0 0;text-shadow:0 1px #fff;font-family:Consolas,Monaco,"Andale Mono","Ubuntu Mono",monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;hyphens:none}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{text-shadow:none;background:#b3d4fc}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{text-shadow:none;background:#b3d4fc}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{padding:1em;margin:.5em 0;overflow:auto}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{color:#9a6e3a;background:hsla(0,0%,100%,.5)}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.nuxt-progress{position:fixed;top:0;left:0;right:0;height:2px;width:0;opacity:1;transition:width .1s,opacity .4s;background-color:#000;z-index:999999}.nuxt-progress.nuxt-progress-notransition{transition:none}.nuxt-progress-failed{background-color:red}</style><link rel="preload" href="/_nuxt/static/1772185199/home%20copy/state.js" as="script"><link rel="preload" href="/_nuxt/static/1772185199/home%20copy/payload.js" as="script"><link rel="preload" href="/_nuxt/static/1772185199/manifest.js" as="script">
  </head>
  <body>
    <div data-server-rendered="true" id="__nuxt"><!----><div id="__layout"><div><div class="outercontainer"><header><div class="container header"><div class="ftheader text"><a href="#home">Hao Yin</a></div> <div class="ftsubheader text publication"><a href="#publications">Publications</a></div> <div class="ftsubheader text talk"><a href="#talks">Talks</a></div> <div class="ftsubheader text teaching"><a href="#teaching">Teaching</a></div> <div class="ftsubheader text group"><a href="#group">Group</a></div> <div class="ftsubheader text"><a href="#home">Home</a></div></div></header> <div class="container body"><div id="home" class="content heading anchor"><div class="img"><img src="/assets/HaoYin.jpg" alt="Photo" class="img_responsive"></div> <div class="text info"><h1>Hao Yin</h1> <p></p> <div>Assistant Professor</div> <div>Computer Science Department</div> <div>Stanford University</div> <div>Email: jiajunwu [at] cs (dot) stanford (dot) edu</div> <p><span><a href="https://scholar.google.com/citations?user=2efgcS0AAAAJ&hl=en&oi=ao">Google Scholar</a></span>
              /
              <span><a href="http://dblp.dagstuhl.de/pers/hd/w/Wu_0001:Jiajun">DBLP</a></span>
              /
              <span><a href="https://profiles.stanford.edu/jiajun-wu">Stanford Profiles</a></span></p> <p></p></div> <div class="text topic"><p></p> <div class="title">Current Research Topics</div> <ul><li><a href="#phys" onclick="showPubs(2)">Physical Scene Understanding</a></li> <li><a href="#robot" onclick="showPubs(2)">Dynamics Models</a></li> <li><a href="#reason" onclick="showPubs(2)">Neuro-Symbolic Visual Reasoning</a></li> <li><a href="#gen" onclick="showPubs(2)">Generative Visual Models</a></li> <li><a href="#multi" onclick="showPubs(2)">Multi-Modal Perception</a></li></ul> <p></p></div> <div class="text"><p>
              I am an Assistant Professor of
              <a href="https://cs.stanford.edu/">Computer Science</a> at
              <a href="https://www.stanford.edu/">Stanford University</a>,
              affiliated with the
              <a href="http://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a>
              and the
              <a href="https://ai.stanford.edu/">Stanford AI Lab (SAIL)</a>. I
              study machine perception, reasoning, and interaction with the
              physical world, drawing inspiration from human cognition.

              <a href="javascript:showRecruit()">Here is some information for
                <span class="highlight">prospective students and visitors</span>.</a></p> <div id="recruit" class="recruit"><p>
                Thank you for your interest in joining my group! Due to the
                large number of emails I receive, I cannot respond to every
                email individually. Please review the information below before
                contacting me. Thanks.
              </p> <p><span class="highlight">Current Stanford students</span>: please
                fill out
                <a href="https://forms.gle/TuCU51tLBpZimyCB8">this form</a>. For
                MS students and undergraduates, the minimum time commitment is
                15 hours per week for six months.
              </p> <p><span class="highlight">Prospective postdocs and visiting graduate students</span>: please email me directly with your CV. For visiting graduate
                students, the minimum length of a research visit is six months.
              </p> <p><span class="highlight">Prospective graduate students who are not currently at
                  Stanford</span>: please apply through the system and list me as a potential
                advisor in your application. There is no need to contact me,
                unless you have a particular research question/idea that you
                would like to discuss further. If you email me, please describe
                the research question concretely, and include your CV,
                transcript, and no more than one published paper.
              </p></div> <p>
              Before joining Stanford, I was a Visiting Faculty Researcher at
              <a href="https://ai.google/">Google Research, New York City</a>,
              working with
              <a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a>. I
              finished my PhD at MIT, advised by
              <a href="http://billf.mit.edu/">Bill Freeman</a> and
              <a href="http://web.mit.edu/cocosci/josh.html">Josh Tenenbaum</a>,
              and my undergraduate degrees at Tsinghua University, working with
              <span class="author"><a href="http://pages.ucsd.edu/~ztu/">Zhuowen Tu</a></span>.
            </p></div></div> <div id="group" class="content group anchor"><div class="text front"><h3>Group</h3> <div class="title">Postdocs</div> <ul><li><a href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a> (with
                <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a>
                and
                <a href="https://profiles.stanford.edu/silvio-savarese">Silvio Savarese</a>)
              </li> <li><a href="https://yunzhuli.github.io/">Yunzhu Li</a> (with
                <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a>)
              </li> <li><a href="https://ai.stanford.edu/~zharu/">Ruohan Zhang</a> (with
                <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a>
                and
                <a href="https://profiles.stanford.edu/silvio-savarese">Silvio Savarese</a>)
              </li></ul> <div class="title">PhD Students</div> <ul><li><a href="https://samuelpclarke.com/">Samuel Clarke</a></li> <li><a href="https://ceyzaguirre4.github.io/">Cristobal Eyzaguirre</a>
                (with <a href="http://www.niebles.net/">Juan Carlos Niebles</a>)
              </li> <li><a href="https://shellguo.com/">Michelle Guo</a> (with
                <a href="https://tml.stanford.edu/people/karen-liu">Karen Liu</a>)
              </li> <li><a href="http://web.stanford.edu/~joycj/">Joy Hsu</a></li> <li><a href="https://kylehsu.org/">Kyle Hsu</a> (with
                <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>)
              </li> <li><a href="https://cs.stanford.edu/~sumith/">Sumith Kulal</a>
                (with
                <a href="http://theory.stanford.edu/~aiken/">Alex Aiken</a>)
              </li> <li><a href="http://lijiaman.github.io/">Jiaman Li</a> (with
                <a href="https://tml.stanford.edu/people/karen-liu">Karen Liu</a>)
              </li> <li><a href="https://mlingelbach.com/">Michael Lingelbach</a> (with
                <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a>)
              </li> <li><a href="https://s-tian.github.io/">Stephen Tian</a></li> <li><a href="https://kovenyu.com/">Koven Yu</a></li> <li><a href="https://ai.stanford.edu/~yzzhang/">Yunzhi Zhang</a></li></ul> <div class="title">Alumnus</div> <ul><li><a href="http://hxu.rocks/">Huazhe Xu</a></li></ul></div></div> <div id="teaching" class="content teaching anchor"><div class="text front"><h3>Teaching</h3> <ul><li>
                SYMSYS1/SYMSYS200/CS24/LINGUIST35/PHIL99/PSYCH35: Minds and
                Machines, Fall 2022 (with
                <a href="https://web.stanford.edu/~icard/">Thomas Icard</a>)
              </li> <li><a href="http://cs231n.stanford.edu/">CS231N: Deep Learning for Computer Vision, Spring 2022</a>
                (with
                <a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a>
                and <a href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>)
              </li> <li><a href="https://cs322.stanford.edu/">PSYCH225/CS322: Triangulating Intelligence: Melding
                  Neuroscience, Psychology, and AI, Winter 2022</a>
                (with
                <a href="http://web.stanford.edu/~hyo/Home.html">Hyo Gweon</a>
                and <a href="https://web.stanford.edu/~yamins/">Dan Yamins</a>)
              </li> <li><a href="http://cs348i.stanford.edu/">CS348I: Computer Graphics in the Era of AI, Fall 2021, Fall
                  2020</a>
                (with
                <a href="https://tml.stanford.edu/people/karen-liu">Karen Liu</a>)
              </li> <li><a href="https://stanford-cs221.github.io/winter2021/">CS221: Artificial Intelligence: Principles and Techniques,
                  Winter 2021</a>
                (with <a href="https://thashim.github.io/">Tatsu Hashimoto</a>)
              </li> <li><a href="http://vision.stanford.edu/teaching/cs131_fall2021/">CS131: Computer Vision: Foundations and Applications, Fall
                  2020</a>
                (with <a href="http://www.niebles.net/">Juan Carlos Niebles</a>)
              </li></ul></div></div> <div id="talks" class="content talk anchor"><div class="text front"><h3>Talks</h3> <span>
              Naturally Supervised Vision (July 2022)
              <div src="/assets/y_DNZTDmA0s.html" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></div></span> <span>
              Multi-Sensory Neural Objects (Oct 2022)
              <div src="/assets/6b5M0kx_MKg.html" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></div></span> <span>
              Research Overview (Apr 2021)
              <div src="/assets/0c6bEyXQ898.html" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></div></span></div></div> <div id="publications" class="content anchor"><div class="text front"><h3>
              Publications (
              <a href id="select0" onclick="return showPubs(0),!1" style="text-decoration:underline;color:#000">show selected</a>
              /
              <a href id="select1" onclick="return showPubs(1),!1">show all by date</a>
              /
              <a href id="select2" onclick="return showPubs(2),!1">show all by topic</a>)
            </h3> <div><span class="venue">Year:</span> <span class="tag"><a href="#2022" onclick="showPubs(1)">2022</a></span>
              /
              <span class="tag"><a href="#2021" onclick="showPubs(1)">2021</a></span>
              /
              <span class="tag"><a href="#2020" onclick="showPubs(1)">2020</a></span>
              /
              <span class="tag"><a href="#2019" onclick="showPubs(1)">2019</a></span>
              /
              <span class="tag"><a href="#2018" onclick="showPubs(1)">2018</a></span>
              /
              <span class="tag"><a href="#2017" onclick="showPubs(1)">2017</a></span>
              /
              <span class="tag"><a href="#2016" onclick="showPubs(1)">2016</a></span>
              /
              <span class="tag"><a href="#2015" onclick="showPubs(1)">2015</a></span>
              /
              <span class="tag"><a href="#2014" onclick="showPubs(1)">2014 and before</a></span></div> <div><span class="venue">Current Research Topics:</span> <span class="tag"><a href="#phys" onclick="showPubs(2)">Physical Scene Understanding</a></span>
              /
              <span class="tag"><a href="#robot" onclick="showPubs(2)">Dynamics Models</a></span>
              /
              <span class="tag"><a href="#reason" onclick="showPubs(2)">Neuro-Symbolic Visual Reasoning</a></span>
              /
              <span class="tag"><a href="#gen" onclick="showPubs(2)">Generative Visual Models</a></span>
              /
              <span class="tag"><a href="#multi" onclick="showPubs(2)">Multi-Modal Perception;</a></span> <span class="venue">Past Research Topic:</span> <span class="tag"><a href="#wsl" onclick="showPubs(2)">Weakly-Supervised Learning</a></span></div> <div><span class="tag">(* and � indicate equal contribution or alphabetical
                order)</span></div></div> <div id="pubs"><div class="text anchor"> </div> <div class="publication"><div class="img"><img src="/assets/spotlight_objectfolder2.jpg" alt="spotlight_objectfolder2" class="img_responsive"></div> <div class="text"><div class="title"><a name="objectfolder2_cvpr" href="papers/objectfolder2_cvpr.pdf">ObjectFolder 2.0: A Multisensory Object Dataset for
                    Sim2Real Transfer</a></div> <div class="authors"><span class="author"><a href="https://ai.stanford.edu/~rhgao/">Ruohan Gao</a>*</span>,
                  <span class="author"><a href="https://si-lynnn.github.io/">Zilin Si</a>*</span>,
                  <span class="author"><a href="https://yuyuchang.github.io/">Yen-Yu Chang</a>*</span>,
                  <span class="author"><a href="https://samuelpclarke.com/">Samuel Clarke</a></span>,
                  <span class="author"><a href="http://web.stanford.edu/~bohg/">Jeannette Bohg</a></span>,
                  <span class="author"><a href="https://profiles.stanford.edu/fei-fei-li/">Li Fei-Fei</a></span>,
                  <span class="author"><a href="http://robotouch.ri.cmu.edu/yuanwz/">Wenzhen Yuan</a></span>,
                  <span class="author jw">Hao Yin</span></div> <div><span class="venue"><a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></span>
                  /
                  <span class="tag"><a href="papers/objectfolder2_cvpr.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="https://ai.stanford.edu/~rhgao/objectfolder2.0/">Project Page</a></span></div></div></div> <div class="publication"><div class="img"><video autoplay muted loop webkit-playsinline playsinline class="img_responsive"><source src="images/spotlight_behavior1k.mp4" type="video/mp4">
                  spotlight_behavior1k
                </video></div> <div class="text"><div class="title"><a name="behavior1k_corl" href="papers/behavior1k_corl.pdf">BEHAVIOR-1K: A Benchmark for Embodied AI with 1,000
                    Everyday Activities and Realistic Simulation</a></div> <div class="authors"><span class="author"><a href="https://www.chengshuli.me/">Chengshu Li</a>*</span>,
                  <span class="author"><a href="https://ai.stanford.edu/~zharu/">Ruohan Zhang</a>*</span>,
                  <span class="author"><a href="https://www.jowo.me/">Josiah Wong</a>*</span>,
                  <span class="author"><a href="https://www.cemgokmen.com/">Cem Gokmen</a>*</span>, <span class="author">Sanjana Srivastava*</span>,
                  <span class="author"><a href="https://robertomartinmartin.com/">Roberto Martin-Martin</a>*</span>,
                  <span class="author"><a href="https://www.chenwangjeremy.net/">Chen Wang</a>*</span>,
                  <span class="author"><a href="https://www.gabrael.io/">Gabrael Levine</a>*</span>,
                  <span class="author"><a href="https://mlingelbach.com/">Michael Lingelbach</a></span>,
                  <span class="author"><a href="http://web.stanford.edu/~jksun/">Jiankai Sun</a></span>, <span class="author">Mona Anvari</span>,
                  <span class="author"><a href="https://mj-hwang.github.io/">Minjune Hwang</a></span>, <span class="author">Manasi Sharma</span>,
                  <span class="author">Arman Aydin</span>,
                  <span class="author"><a href="https://dhruvabansal.com/">Dhruva Bansal</a></span>, <span class="author">Samuel Hunter</span>,
                  <span class="author"><a href="https://kykim0.github.io/">Kyu-Young Kim</a></span>, <span class="author">Alan Lou</span>,
                  <span class="author">Caleb R. Matthews</span>,
                  <span class="author">Ivan Villa-Renteria</span>,
                  <span class="author">Jerry Huayang Tang</span>,
                  <span class="author">Claire Tang</span>,
                  <span class="author"><a href="https://fxia22.github.io/">Fei Xia</a></span>,
                  <span class="author"><a href="https://profiles.stanford.edu/silvio-savarese">Silvio Savarese</a></span>,
                  <span class="author"><a href="https://web.stanford.edu/~hyo/Home.html">Hyowon Gweon</a></span>,
                  <span class="author"><a href="https://tml.stanford.edu/people/karen-liu">C. Karen Liu</a></span>, <span class="author jw">Hao Yin</span>,
                  <span class="author"><a href="https://profiles.stanford.edu/fei-fei-li/">Li Fei-Fei</a></span></div> <div><span class="venue"><a href="https://www.robot-learning.org/">CoRL 2022</a></span>
                  /
                  <span class="tag"><a href="papers/behavior1k_corl.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="https://behavior.stanford.edu/">Project Page</a></span></div> <div><span class="highlight">Oral Presentation</span></div></div></div> <div class="publication"><div class="img"><video autoplay muted loop webkit-playsinline playsinline class="img_responsive"><source src="images/spotlight_pvd.mp4" type="video/mp4">
                  spotlight_pvd
                </video></div> <div class="text"><div class="title"><a name="pvd_iccv" href="papers/pvd_iccv.pdf">3D Shape Generation and Completion Through Point-Voxel
                    Diffusion</a></div> <div class="authors"><span class="author"><a href="https://alexzhou907.github.io/">Linqi Zhou</a></span><a href="https://alexzhou907.github.io/">, <span class="author"></span></a><a href="https://yilundu.github.io/">Yilun Du</a>,
                  <span class="author jw">Hao Yin</span></div> <div><span class="venue"><a href="http://iccv2021.thecvf.com/home">ICCV 2021</a></span>
                  /
                  <span class="tag"><a href="papers/pvd_iccv.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="https://alexzhou907.github.io/pvd">Project Page</a></span></div> <div><span class="highlight">Oral Presentation</span></div></div></div> <div class="publication"><div class="img"><video autoplay muted loop webkit-playsinline playsinline class="img_responsive"><source src="images/spotlight_bpi.mp4" type="video/mp4">
                  spotlight_bpi
                </video></div> <div class="text"><div class="title"><a name="bpi_nips" href="papers/bpi_nips.pdf">Multi-Plane Program Induction with 3D Box Priors</a></div> <div class="authors"><span class="author">Yikai Li*</span>,
                  <span class="author"><a href="http://jiayuanm.com/">Jiayuan Mao</a>*</span>,
                  <span class="author"><a href="https://xiuming.info/">Xiuming Zhang</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span>,
                  <span class="author"><a href="http://www.cs.cornell.edu/~snavely/">Noah Snavely</a></span>,
                  <span class="author jw">Hao Yin</span></div> <div><span class="venue"><a href="https://nips.cc/Conferences/2020">NeurIPS 2020</a></span>
                  /
                  <span class="tag"><a href="papers/bpi_nips.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://bpi.csail.mit.edu/">Project Page</a></span></div></div></div> <div class="publication"><div class="img"><img src="/assets/spotlight_dissertation.png" alt="spotlight_dissertation" class="img_responsive"></div> <div class="text"><div class="title"><a name="dissertation" href="papers/dissertation.pdf">Learning to See the Physical World</a></div> <div><span class="venue"><a href="papers/dissertation.pdf">PhD Dissertation 2019</a></span></div> <div><span class="highlight"><a href="https://awards.acm.org/about/2019-doctoral-dissertation">ACM Doctoral Dissertation Award Honorable Mention</a></span><br> <span class="highlight">Joint AAAI/ACM SIGAI Doctoral Dissertation Award</span><br> <span class="highlight">MIT George M. Sprowls PhD Thesis Award in Artificial
                    Intelligence and Decision-Making</span></div></div></div> <div class="publication"><div class="img"><img src="/assets/spotlight_nscl.jpg" alt="spotlight_nscl" class="img_responsive"></div> <div class="text"><div class="title"><a name="nscl_iclr" href="papers/nscl_iclr.pdf">The Neuro-Symbolic Concept Learner:<br>Interpreting
                    Scenes, Words, and Sentences from Natural Supervision</a></div> <div class="authors"><span class="author"><a href="http://jiayuanm.com/">Jiayuan Mao</a></span>,
                  <span class="author"><a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a></span>,
                  <span class="author"><a href="https://sites.google.com/site/pushmeet/">Pushmeet Kohli</a></span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span>,
                  <span class="author jw">Hao Yin</span></div> <div><span class="venue"><a href="https://openreview.net/forum?id=rJgMlhRctm">ICLR 2019</a></span>
                  /
                  <span class="tag"><a href="papers/nscl_iclr.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://nscl.csail.mit.edu/">Project Page</a></span>
                  /
                  <span class="tag"><a href="http://news.mit.edu/2019/teaching-machines-to-reason-about-what-they-see-0402">MIT News</a></span>
                  /
                  <span class="tag"><a href="https://www.technologyreview.com/s/613270/two-rival-ai-approaches-combine-to-let-machines-learn-about-the-world-like-a-child/">MIT Technology Review</a></span></div> <div><span class="highlight">Oral Presentation</span></div></div></div> <div class="publication"><div class="img"><img src="/assets/spotlight_shape2prog.jpg" alt="spotlight_shape2prog" class="img_responsive"></div> <div class="text"><div class="title"><a name="shape2prog_iclr" href="papers/shape2prog_iclr.pdf">Learning to Infer and Execute 3D Shape Programs</a></div> <div class="authors"><span class="author"><a href="http://people.csail.mit.edu/yonglong/">Yonglong Tian</a></span>, <span class="author">Andrew Luo</span>,
                  <span class="author"><a href="https://xingyuansun.com/">Xingyuan Sun</a></span>,
                  <span class="author"><a href="https://www.cs.cornell.edu/~ellisk/">Kevin Ellis</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span>,
                  <span class="author jw">Hao Yin</span></div> <div><span class="venue"><a href="https://openreview.net/forum?id=rylNH20qFQ">ICLR 2019</a></span>
                  /
                  <span class="tag"><a href="papers/shape2prog_iclr.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://shape2prog.csail.mit.edu/">Project Page</a></span></div></div></div> <div class="publication"><div class="img"><video autoplay muted loop webkit-playsinline playsinline class="img_responsive"><source src="images/spotlight_vd.mp4" type="video/mp4">
                  spotlight_vd
                </video></div> <div class="text"><div class="title"><a name="vd_tpami" href="papers/vd_tpami.pdf">Visual Dynamics: Stochastic Future Generation via Layered
                    Cross Convolutional Networks</a></div> <div class="authors"><span class="author"><a href="https://tianfan.info/">Tianfan Xue</a>*</span>, <span class="author jw">Hao Yin*</span>,
                  <span class="author"><a href="http://users.cms.caltech.edu/~klbouman/">Katherine L. Bouman</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span></div> <div><span class="venue"><a href="https://ieeexplore.ieee.org/document/8409321/">TPAMI 2019</a></span>
                  /
                  <span class="tag"><a href="papers/vd_tpami.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://visualdynamics.csail.mit.edu/">Project Page</a></span></div> <div><span class="venue"><a href="https://nips.cc/Conferences/2016">NeurIPS 2016</a></span>
                  /
                  <span class="tag"><a href="papers/vd_nips.pdf">Paper</a></span></div> <div><span class="highlight">Oral Presentation</span></div></div></div> <div class="publication"><div class="img"><img src="/assets/spotlight_genre.jpg" alt="spotlight_genre" class="img_responsive"></div> <div class="text"><div class="title"><a name="genre_nips" href="papers/genre_nips.pdf">Learning to Reconstruct Shapes from Unseen Classes</a></div> <div class="authors"><span class="author"><a href="https://xiuming.info/">Xiuming Zhang</a>*</span>,
                  <span class="author"><a href="https://ztzhang.info/">Zhoutong Zhang</a>*</span>, <span class="author">Chengkai Zhang</span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span>,
                  <span class="author jw">Hao Yin</span></div> <div><span class="venue"><a href="https://nips.cc/Conferences/2018">NeurIPS 2018</a></span>
                  /
                  <span class="tag"><a href="papers/genre_nips.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://genre.csail.mit.edu/">Project Page</a></span></div> <div><span class="highlight">Oral Presentation</span></div></div></div> <div class="publication"><div class="img"><img src="/assets/spotlight_3dinn.jpg" alt="Spotlight_3dinn" class="img_responsive"></div> <div class="text"><div class="title"><a name="3dinn_ijcv" href="papers/3dinn_ijcv.pdf">3D Interpreter Networks for Viewer-Centered Wireframe
                    Modeling</a></div> <div class="authors"><span class="author jw">Hao Yin*</span>,
                  <span class="author"><a href="https://tianfan.info/">Tianfan Xue</a>*</span>,
                  <span class="author"><a href="https://clvrai.com/web_lim/">Joseph J. Lim</a></span>,
                  <span class="author"><a href="http://yuandong-tian.com/">Yuandong Tian</a></span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span>,
                  <span class="author"><a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span></div> <div><span class="venue"><a href="https://link.springer.com/article/10.1007/s11263-018-1074-6">IJCV 2018</a></span>
                  /
                  <span class="tag"><a href="papers/3dinn_ijcv.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://3dinterpreter.csail.mit.edu/">Project Page</a></span></div> <div><span class="venue"><a href="https://www.eccv2016.org/">ECCV 2016</a></span>
                  /
                  <span class="tag"><a href="papers/3dinn_eccv.pdf">Paper</a></span></div> <div><span class="highlight">Oral Presentation</span></div></div></div> <div class="publication"><div class="img"><video autoplay muted loop webkit-playsinline playsinline class="img_responsive"><source src="images/spotlight_vda.mp4" type="video/mp4">
                  spotlight_vda
                </video></div> <div class="text"><div class="title"><a name="vda_nips" href="papers/vda_nips.pdf">Learning to See Physics via Visual De-animation</a></div> <div class="authors"><span class="author jw">Hao Yin</span>,
                  <span class="author"><a href="https://erikalu.com/">Erika Lu</a></span>,
                  <span class="author"><a href="https://sites.google.com/site/pushmeet/">Pushmeet Kohli</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span></div> <div><span class="venue"><a href="https://nips.cc/Conferences/2017">NeurIPS 2017</a></span>
                  /
                  <span class="tag"><a href="papers/vda_nips.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://vda.csail.mit.edu/">Project Page</a></span>
                  /
                  <span class="tag"><a href="http://news.mit.edu/2017/computer-systems-predict-objects-responses-physical-forces-1214">MIT News</a></span></div> <div><span class="highlight">Spotlight Presentation</span></div></div></div> <div class="publication"><div class="img"><img src="/assets/spotlight_marrnet.jpg" alt="Spotlight_marrnet" class="img_responsive"></div> <div class="text"><div class="title"><a name="marrnet_nips" href="papers/marrnet_nips.pdf">MarrNet: 3D Shape Reconstruction via 2.5D Sketches</a></div> <div class="authors"><span class="author jw">Hao Yin*</span>,
                  <span class="author"><a href="https://homes.cs.washington.edu/~yifan1/">Yifan Wang</a>*</span>,
                  <span class="author"><a href="https://tianfan.info/">Tianfan Xue</a></span>,
                  <span class="author"><a href="https://xingyuansun.com/">Xingyuan Sun</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span></div> <div><span class="venue"><a href="https://nips.cc/Conferences/2017">NeurIPS 2017</a></span>
                  /
                  <span class="tag"><a href="papers/marrnet_nips.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://marrnet.csail.mit.edu/">Project Page</a></span></div></div></div> <div class="publication"><div class="img"><img src="/assets/spotlight_3dgan.jpg" alt="Spotlight_3dgan" class="img_responsive"></div> <div class="text"><div class="title"><a name="3dgan_nips" href="papers/3dgan_nips.pdf">Learning a Probabilistic Latent Space of Object Shapes via
                    3D Generative-Adversarial Modeling</a></div> <div class="authors"><span class="author jw">Hao Yin*</span>,
                  <span class="author">Chengkai Zhang*</span>,
                  <span class="author"><a href="https://tianfan.info/">Tianfan Xue</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span></div> <div><span class="venue"><a href="https://nips.cc/Conferences/2016">NeurIPS 2016</a></span>
                  /
                  <span class="tag"><a href="papers/3dgan_nips.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://3dgan.csail.mit.edu/">Project Page</a></span>
                  /
                  <span class="tag"><a href="https://www.technologyreview.com/s/603107/ai-begins-to-understand-the-3-d-world/">MIT Technology Review</a></span></div></div></div> <div class="publication"><div class="img"><img src="/assets/spotlight_galileo.jpg" alt="Spotlight_galileo" class="img_responsive"></div> <div class="text"><div class="title"><a name="galileo_nips" href="papers/galileo_nips.pdf">Galileo: Perceiving Physical Object Properties by
                    Integrating a Physics Engine with Deep Learning</a></div> <div class="authors"><span class="author jw">Hao Yin*</span>,
                  <span class="author"><a href="https://psychology.yale.edu/people/ilker-yildirim">Ilker Yildirim</a>*</span>,
                  <span class="author"><a href="https://clvrai.com/web_lim/">Joseph J. Lim</a></span>,
                  <span class="author"><a href="http://billf.mit.edu/">William T. Freeman</a></span>,
                  <span class="author"><a href="http://web.mit.edu/cocosci/josh.html">Joshua B. Tenenbaum</a></span></div> <div><span class="venue"><a href="https://nips.cc/Conferences/2015">NeurIPS 2015</a></span>
                  /
                  <span class="tag"><a href="papers/galileo_nips.pdf">Paper</a></span>
                  /
                  <span class="tag"><a href="http://galileo.csail.mit.edu/">Project Page</a></span>
                  /
                   <span class="tag"><a href="http://news.mit.edu/2016/csail-computer-model-matches-humans-predicting-how-objects-move-0104">MIT News</a></span>
                  /
                  <span class="tag"><a href="http://motherboard.vice.com/read/how-to-teach-a-robot-to-build-a-rube-goldberg-machine">VICE</a></span>
                  /
                  <span class="tag"><a href="http://www.wired.com/2016/01/mit-researchers-are-figuring-out-how-robots-can-wash-dishes/">WIRED</a></span>
                  /
                  <span class="tag"><a href="http://www.engadget.com/2016/01/05/mit-galileo-object-predictions/">Engadget</a></span></div></div></div></div></div></div></div></div></div></div><script defer src="/_nuxt/static/1772185199/home%20copy/state.js"></script><script src="/_nuxt/cd4766f.js" defer></script><script src="/_nuxt/3c593d9.js" defer></script><script src="/_nuxt/1765d2c.js" defer></script><script src="/_nuxt/0dc6b56.js" defer></script><script src="/_nuxt/e86a509.js" defer></script>
  </body>
</html>
